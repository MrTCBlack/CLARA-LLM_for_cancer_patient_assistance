# This is the evaluation script for evaluating the performance of the CLARA model
# 
# This script evaluates CLARA on 4 text generation metrics:
#   - Bilingual Evaluation Understudy (BLEU)
#   - Recall-Oriented Understudy for Gisting Evaluation (ROGUE)
#   - Metric for Evaluation of Translation with Explicit ORdering (METEOR)
#   - BERTScore
# 
# BLEU - Mesures the precentage of words/phrases in prediction that also appear in the reference
# 1-gram is 1 word; 2-gram is two words; etc.
# BLEU is a geometric average of all 4 grams multiplied by the brevity penalty
# Brevity penalty punishes the score if prediction is shorter than the reference (penalizes paraphrasing)
# Not the best metric for evaluating long answer responses generated by model.
# We want the model to be using some of the same detailed words as the reference.
# An increase in this metric would show that the model is learning those medical specific words,
#   or at least learning to use them in specific contexts
# Given that, the most important metric to look at is BLEU 1-gram 
# 
# ROUGE - Looking at how much of the reference appears in the prediction
# More tolerant the BLEU of reordering, partially correct content, 
#   and long answers where not everything needs to match
# ROUGE-1 looks for overlap of individual words.
# ROUGE-2 looks for overlap of 2-word sequences
# ROUGE-L looks for longest common subsequece - captures fluency and ordering
# ROUGE focusses on did the model cover the key information from the reference and
#   are the same content words present.
# Can use it to measure the similarity between the reference and the output.
# More specifically, allows us to see if the important content from the reference
#   is also in the output
# Use in tandom with BLEU to measure the overlap of words
# ROUGE1 should be close to BLEU 1-gram; ROUGE2 should be very close to BLEU 2-gram 
# As with BLEU, and increase in this metric is showing that the model is learning to
#   use the correct medical words in specific contexts
# 
# METEOR - Improves over BLEU by incorporating synonym matching, stemming,
#   and word order penalties
# It aligns words between prediction and reference using exact matches,
#   stem matches, synonym matches.
# Then computes score based on precision and recall over aligned chunks
# Captures semantic similarity and allows more flexible phrasing
# METEOR can help judge overlap in content and meaning better than BLEU and ROUGE
# METEOR splits BLEU/ROUGE and BERTScore where it still cares about word order
#   but incorporates semantics and meaning into it's score
# 
# BERTScore - Uses a pretrained language model (e.g., BERT) to compute semantic similarity
#               between tokens in the prediciton and reference
# Compares contextualized word embeddings rather than raw tokens
# Captues semantic similarity and works well with paraphrasing
# Precision - How many tokens in the prediction are semantically aligned
#               with the reference
# Recall - How many tokens in the reference are found (semantically matched)
#           in the prediciton
# F1 - The harmonic mean of precision and recall - a balanced score
#       representing overall semantic overlap 
# One of the best metrics to actually evaluate if what the model is outputting
#   is related to the reference
# 
# Will be evaluating the model on how well it answers questions from the lavita/MedQuAD dataset
#   that contain the word 'cancer' in the question.
# This is a subset of 70 form the lavita/MedQuAD dataset and will not be used for training
# Each question has expert verified answers.
# That is why it is appropriate to use them to compare against the output of our model  



from datasets import load_dataset, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from transformers import Trainer, TrainingArguments, EvalPrediction
from transformers import DataCollatorWithPadding
from transformers import BitsAndBytesConfig
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from copy import deepcopy
import torch
import numpy as np
import evaluate
import threading
import sys
import os

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token

def load_eval_set():
    # Load lavita/MedQuAD dataset
    medQuAD = load_dataset("lavita/MedQuAD")

    #Extract the train split
    # Size: 47441
    medQuAD = medQuAD["train"]

    # Remove all examples that don't contain an answer
    # Size: 16407
    medQuAD = medQuAD.filter(lambda example: example["answer"]!=None)

    # Important parts of this dataset:
    #   - question: The question being ask
    #   - answer: The long response answer to the question

    # Keep only records where the question contains the word 'cancer'
    # Size: 70
    evaluation_set = medQuAD.filter(lambda example: example["question"].find("cancer")!= -1)


    #print(evaluation_set["question"])
    return evaluation_set


def load_model(adapter_path):
    # Run Evaluation with Trainer
    quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True)

    # Load Mistral model
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/mistral-7b-instruct-v0.3",
        device_map="auto",
        quantization_config=quantization_config,
        #torch_dtype="auto",
        trust_remote_code=False,
        max_memory={
            0: "4GiB",
            "cpu": "28GiB"
        }
    )

    model.config.pad_token_id = tokenizer.pad_token_id


    not_done = True
    while not_done:
        user_input = input("Are you evaluating before training? ('yes' or 'no')")
        match user_input:
            case "yes":
               # create LoRA configuration object
                lora_config = LoraConfig(
                    task_type=TaskType.CAUSAL_LM, # type of task to train on
                    inference_mode=False, # set to False for training
                    r=8, # dimension of the smaller matrices
                    lora_alpha=32, # scaling factor
                    lora_dropout=0.1 # dropout of LoRA layers
                )

                # Add LoraConfig to the model
                model = get_peft_model(model, lora_config)
                not_done = False 
            case "no":
                # Load in adapter from save directoyr
                model = PeftModel.from_pretrained(model, adapter_path)
                not_done = False
            case _:
                continue

    # Need to call this anytime the adapter is loaded
    # Esures:
    #   - The LoRA weights are active
    #   - Their gradients are enabled (requires_grad = True)
    #   - You're not accidentally training a frozen model 
    model.set_adapter("default")
    
    return model


def preprocess_evaluation_function(examples):
    """
    Preprocesses a QA example for *testing* using open-ended format.
    Only contains the question so that the model can generate the answer
    Don't care about loss so no need for masking except for padding.
    Also returns gold answer for external metrics like accuracy or F1.
    """

    processed = {
        "input_ids": [],
        "attention_mask": [],
        "gold_answer_text": [],
    }

    prompts = []

    for i in range(len(examples["question"])):
        #choices_block = "\n".join([f"{key}. {choice}" for key, choice in examples["options"][i].items()])
        prompt = f"Question:\n{examples['question'][i]}\n\nAnswer:\n"
        prompts.append(prompt)
        gold_answer_text = examples["answer"][i]

        processed["gold_answer_text"].append(gold_answer_text)


    # Tokenize
    tokenized = tokenizer(
        prompts,
        add_special_tokens=True,
        return_tensors=None,)
    
    processed["input_ids"] = tokenized["input_ids"]
    processed["attention_mask"] = tokenized["attention_mask"]


    return processed


# This is the evaluation for metrics of what the model generates
def evaluate_model_generation(model, tokenized_evaluation_set):
    model.eval()
    eval_dataset = deepcopy(tokenized_evaluation_set)
    eval_dataset = eval_dataset.remove_columns(["gold_answer_text"])

    all_preds = []
    gold_answers = [answer.strip() for answer in tokenized_evaluation_set["gold_answer_text"]]
    questions = []

    for i in range(len(eval_dataset["input_ids"])):
        input_ids = torch.tensor(eval_dataset["input_ids"][i]).unsqueeze(0).to(model.device)
        attention_mask = torch.tensor(eval_dataset["attention_mask"][i]).unsqueeze(0).to(model.device)

        question = tokenizer.decode(eval_dataset["input_ids"][i])
        questions.append(question)
        print(question)

        # NOTE: Streamer is for if you want to see what it is outputting before
        #   it is postprocessed to compare to the expected value
        '''streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        thread = threading.Thread(target=model.generate, kwargs={
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "max_new_tokens": 500,
            "streamer": streamer,
            # If you are seeing that the model is starting to repeat itself and not
            #   fully complete an output, turn this flag on
            "do_sample": True,
        })
        thread.start()

        # Collect the output from the streamer
        decoded_tokens = ""
        for token in streamer:
            print(token, end="", flush=True)  # stream to console
            decoded_tokens += token

        print()  # new line

        all_preds.append(decoded_tokens.strip())'''

        # NOTE: This is for if you don't care about seeing the generated output as it's generating it
        generated_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=500,
            # If you are seeing that the model is starting to repeat itself and not
            #   fully complete an output, turn this flag on
            do_sample=True,
        )
        
        new_tokens = generated_ids[0, input_ids.shape[1]:]
        preds = tokenizer.decode(new_tokens, skip_special_tokens=True)

        all_preds.append(preds)

    #correct = []
    print()
    #for i in range(len(all_preds)):
    #    print(f"Prediction: {all_preds[i]}")
    #    print(f"Expected: {gold_answers[i]}\n")

    bleu = evaluate.load('bleu')
    bleu_scores = []
    for pred, ref in zip(all_preds, gold_answers):
        bleu_result = bleu.compute(predictions=[pred],
                                   references=[[ref]],)
        bleu_scores.append(bleu_result)
    #print(bleu_scores)

    rouge = evaluate.load('rouge')
    rouge_results = rouge.compute(predictions=all_preds,
                                  references=gold_answers,
                                  use_aggregator=False)
    #print(rouge_results)

    meteor = evaluate.load("meteor")
    meteor_scores = []
    for pred, ref in zip(all_preds, gold_answers):
        meteor_result = meteor.compute(predictions=[pred],
                                   references=[[ref]])
        meteor_scores.append(meteor_result)
    #print(meteor_scores)


    bertscore = evaluate.load("bertscore")
    bertscore_results = bertscore.compute(predictions=all_preds,
                                            references=gold_answers,
                                            lang="en")
    
    bleu_score = []
    bleu1_score = []
    bleu2_score = []
    bleu3_score = []
    bleu4_score = []
    meteor_score = []
    for i in range(len(questions)):
        bleu_score.append(bleu_scores[i]["bleu"])
        bleu1_score.append(bleu_scores[i]["precisions"][0])
        bleu2_score.append(bleu_scores[i]["precisions"][1])
        bleu3_score.append(bleu_scores[i]["precisions"][2])
        bleu4_score.append(bleu_scores[i]["precisions"][3])
        meteor_score.append(meteor_scores[i]["meteor"])

    avg_bleu = np.mean(bleu_score)
    avg_bleu1 = np.mean(bleu1_score)
    avg_bleu2 = np.mean(bleu2_score)
    avg_bleu3 = np.mean(bleu3_score)
    avg_bleu4 = np.mean(bleu4_score)
    avg_meteor = np.mean(meteor_score)

    aggregate_rouge_results = rouge.compute(predictions=all_preds,
                                  references=gold_answers,
                                  use_aggregator=True)
    avg_rouge1 = aggregate_rouge_results["rouge1"]
    avg_rouge2 = aggregate_rouge_results["rouge2"]
    avg_rougeL = aggregate_rouge_results["rougeL"]
    avg_rougeLsum = aggregate_rouge_results["rougeLsum"]

    avg_bert_precision = np.mean(bertscore_results["precision"])
    avg_bert_recall = np.mean(bertscore_results["recall"])
    avg_bert_f1 = np.mean(bertscore_results["f1"])

    averages = {
        "avg_bleu": avg_bleu,
        "avg_bleu1": avg_bleu1,
        "avg_bleu2": avg_bleu2,
        "avg_bleu3": avg_bleu3,
        "avg_bleu4": avg_bleu4,
        "avg_meteor": avg_meteor,
        "avg_rouge1": avg_rouge1,
        "avg_rouge2": avg_rouge2,
        "avg_rougeL": avg_rougeL,
        "avg_rougeLsum": avg_rougeLsum,
        "avg_bert_precision": avg_bert_precision,
        "avg_bert_recall": avg_bert_recall,
        "avg_bert_f1": avg_bert_f1,
    }


    #print(bertscore_results)

    results = {
        "bleu": bleu_scores,
        "rouge": rouge_results,
        "meteor": meteor_scores,
        "bertscore": bertscore_results,
        "questions": questions,
        "predictions": all_preds,
        "references": gold_answers,
        "averages": averages
    }


    return results


def output_results(results):

    #NOTE: These are the most important metrics. Make sure to save them somewhere
    averages = results["averages"]
    print(f"Average BLEU: {averages["avg_bleu"]}")
    print(f"Average BLEU 1-gram: {averages["avg_bleu1"]}")
    print(f"Average BLEU 2-gram: {averages["avg_bleu2"]}")
    print(f"Average BLEU 3-gram: {averages["avg_bleu3"]}")
    print(f"Average BLEU 4-gram: {averages["avg_bleu4"]}")
    print(f"Average ROUGE-1: {averages["avg_rouge1"]}")
    print(f"Average ROUGE-2: {averages["avg_rouge2"]}")
    print(f"Average ROUGE-L: {averages["avg_rougeL"]}")
    print(f"Average ROUGE-Lsum: {averages["avg_rougeLsum"]}")
    print(f"Average METEOR: {averages["avg_meteor"]}")
    print(f"Average BERTScore Precision: {averages["avg_bert_precision"]}")
    print(f"Average BERTScore Recall: {averages["avg_bert_recall"]}")
    print(f"Average BERTScore F1: {averages["avg_bert_f1"]}")
    print()

    not_done = True
    save_all = False
    while not_done:
        user_input = input("Do you want to save metrics for each question? ('yes' or 'no')")
        match user_input:
            case "yes":
                save_all = True
                not_done = False 
            case "no":
                not_done = False
            case _:
                continue

    if save_all:
        print("Saving output to save_output.txt")
        print("Make sure to move new file after saving.")

        questions = results["questions"]
        predictions = results["predictions"]
        references = results["references"]

        bleu_scores = results["bleu"]
        rouge_scores = results["rouge"]
        meteor_scores = results["meteor"]
        bertscores = results["bertscore"]

        with open("save_output.txt", "w") as file:

            file.write("\n")
            for i in range(len(questions)):
                file.write(f"Question {i+1}:\n{questions[i]}\n\n")
                file.write(f"Prediction:\n{predictions[i]}\n\n")
                file.write(f"Reference:\n{references[i]}\n\n\n")


                file.write(f"BLEU Score: {bleu_scores[i]["bleu"]}\n")
                file.write(f"BLEU 1-gram: {bleu_scores[i]["precisions"][0]}\n")
                file.write(f"BLEU 2-gram: {bleu_scores[i]["precisions"][1]}\n")
                file.write(f"BLEU 3-gram: {bleu_scores[i]["precisions"][2]}\n")
                file.write(f"BLEU 4-gram: {bleu_scores[i]["precisions"][3]}\n")
                file.write(f"Prediction Length: {bleu_scores[i]["translation_length"]}\n")
                file.write(f"Reference Length: {bleu_scores[i]["reference_length"]}\n")
                file.write(f"Brevity Penalty: {bleu_scores[i]["brevity_penalty"]}\n\n")


                file.write(f"ROUGE1: {rouge_scores["rouge1"][i]}\n")
                file.write(f"ROUGE2: {rouge_scores["rouge2"][i]}\n")
                file.write(f"ROUGEL: {rouge_scores["rougeL"][i]}\n")
                file.write(f"ROUGELsum: {rouge_scores["rougeLsum"][i]}\n\n")

    
                file.write(f"METEOR: {meteor_scores[i]["meteor"]}\n\n")

    
                file.write(f"BERTscore Precision: {bertscores["precision"][i]}\n")
                file.write(f"BERTscore Recall: {bertscores["recall"][i]}\n")
                file.write(f"BERTscore f1: {bertscores["f1"][i]}\n\n")

        print("Finished saving metrics.")


'''
There are one command line arguments:
    python Evaluation_Script.py <adapter_directory>
        - adapter_directroy - the directory to save your lora adapter config in 
'''
def main():


    if (len(sys.argv) != 2):
        print("Adapter Directory Path needed.")
        print("run arguments:")
        print("\tpython Evaluation_Script.py <adapter_directory>")
        print("\t- adapter_directory - the directory to save your LoRA adapter config in")
        return
    
    adapter_path = sys.argv[1]
    if not os.path.exists(adapter_path):
        print(f"{adapter_path} does not exist.")
        return
    
    evaluation_set = load_eval_set()
    

    model = load_model(adapter_path)

    #NOTE: Change this if you want less than the max of 70
    #evaluation_set = evaluation_set.select(range(2))


    tokenized_evaluation_set = evaluation_set.map(
        preprocess_evaluation_function, 
        batched=True,
        remove_columns=evaluation_set.column_names)
    
    results = evaluate_model_generation(model, tokenized_evaluation_set)

    output_results(results)

    

main()